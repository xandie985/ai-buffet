{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tool Calling\n",
    "Lets discuss about \"How do agents share information with other agents?\"\n",
    "\n",
    "- Till now, we used LLMs to make a decision by choosing from different pipelines. \n",
    "- Now lets move ahead by demonstrating how to use an LLM to pick a function to execute and infer an argument to pass through the function. \n",
    "- The ability of LLMs to take actions and interact with their external environment is crucial, and a good interface for the LLMs is necessary to make this possible. \n",
    "- Tool calling is a key component of this process.\n",
    "- We use LLMs for tool calling in a basic RAG pipeline and show how to make LLMs pick the best query pipeline to answer user queries.\n",
    "- This process allows LLMs to not only pick a function to execute but also to infer the arguments needed. \n",
    "- This results in users getting more precise answers than standard RAG techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip3 uninstall llama-index\\n!pip3 install llama-index --upgrade --no-cache-dir --force-reinstall\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip3 uninstall llama-index\n",
    "!pip3 install llama-index --upgrade --no-cache-dir --force-reinstall\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import os\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a Simple Tool**\n",
    "\n",
    "Example functions like an `add()` function and a `mystery()` function are defined to illustrate tool calling.\n",
    "The function tool wraps any given Python function, taking in both the add function and the mystery function with *type annotations* and *docstrings* used as prompts for the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def mystery(x: int, y: int) -> int: \n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lets import the LLM module and call the `predict_and_call()` function. \n",
    " - It involves taking in a set of tools and an input prompt string or chat messages, allowing the LLM to decide which tool to call and execute it to get the final response. \n",
    " - An example shows the intermediate steps of calling a function with specific arguments and getting the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of the mystery function on 2 and 9\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Define an Auto-Retrieval Tool**\n",
    "\n",
    "Now let's define a more sophisticated agentic layer on top of vector search. \n",
    "- The LLM can choose vector search and infer metadata filters, which helps return more precise search results. \n",
    "- Using the same paper, GPT2, the lesson focuses on nodes or chunks, demonstrating how metadata attached to chunks, such as page labels, can be used to filter search results.\n",
    "\n",
    "**Steps:**\n",
    "- The process involves using a simple directory reader from LlamaIndex to load the parsed representation of a PDF file, splitting documents into chunks, and looking at the content and metadata of an example chunk. \n",
    "- A vector store index is defined over these nodes, building a RAG indexing pipeline. \n",
    "- The lesson shows how to query this RAG pipeline via metadata filters, specifying filters like page labels to get precise search results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"language_models_are_unsupervised_multitask_learners.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: language_models_are_unsupervised_multitask_learners.pdf\n",
      "file_path: language_models_are_unsupervised_multitask_learners.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 582775\n",
      "creation_date: 2024-06-07\n",
      "last_modified_date: 2024-06-07\n",
      "\n",
      "Language Models are Unsupervised Multitask Learners\n",
      "Alec Radford*1Jeffrey Wu*1Rewon Child1David Luan1Dario Amodei**1Ilya Sutskever**1\n",
      "Abstract\n",
      "Natural language processing tasks, such as ques-\n",
      "tion answering, machine translation, reading com-\n",
      "prehension, and summarization, are typically\n",
      "approached with supervised learning on task-\n",
      "speciﬁc datasets. We demonstrate that language\n",
      "models begin to learn these tasks without any ex-\n",
      "plicit supervision when trained on a new dataset\n",
      "of millions of webpages called WebText. When\n",
      "conditioned on a document plus questions, the an-\n",
      "swers generated by the language model reach 55\n",
      "F1 on the CoQA dataset - matching or exceeding\n",
      "the performance of 3 out of 4 baseline systems\n",
      "without using the 127,000+ training examples.\n",
      "The capacity of the language model is essential\n",
      "to the success of zero-shot task transfer and in-\n",
      "creasing it improves performance in a log-linear\n",
      "fashion across tasks. Our largest model, GPT-2,\n",
      "is a 1.5B parameter Transformer that achieves\n",
      "state of the art results on 7 out of 8 tested lan-\n",
      "guage modeling datasets in a zero-shot setting\n",
      "but still underﬁts WebText. Samples from the\n",
      "model reﬂect these improvements and contain co-\n",
      "herent paragraphs of text. These ﬁndings suggest\n",
      "a promising path towards building language pro-\n",
      "cessing systems which learn to perform tasks from\n",
      "their naturally occurring demonstrations.\n",
      "1. Introduction\n",
      "Machine learning systems now excel (in expectation) at\n",
      "tasks they are trained for by using a combination of large\n",
      "datasets, high-capacity models, and supervised learning\n",
      "(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei\n",
      "et al., 2016). Yet these systems are brittle and sensitive to\n",
      "slight changes in the data distribution (Recht et al., 2018)\n",
      "and task speciﬁcation (Kirkpatrick et al., 2017). Current sys-\n",
      "tems are better characterized as narrow experts rather than\n",
      "*, **Equal contribution1OpenAI, San Francisco, Califor-\n",
      "nia, United States. Correspondence to: Alec Radford\n",
      "<alec@openai.com >.competent generalists. We would like to move towards more\n",
      "general systems which can perform many tasks – eventually\n",
      "without the need to manually create and label a training\n",
      "dataset for each one.\n",
      "The dominant approach to creating ML systems is to col-\n",
      "lect a dataset of training examples demonstrating correct\n",
      "behavior for a desired task, train a system to imitate these\n",
      "behaviors, and then test its performance on independent\n",
      "and identically distributed (IID) held-out examples. This\n",
      "has served well to make progress on narrow experts. But\n",
      "the often erratic behavior of captioning models (Lake et al.,\n",
      "2017), reading comprehension systems (Jia & Liang, 2017),\n",
      "and image classiﬁers (Alcorn et al., 2018) on the diversity\n",
      "and variety of possible inputs highlights some of the short-\n",
      "comings of this approach.\n",
      "Our suspicion is that the prevalence of single task training\n",
      "on single domain datasets is a major contributor to the lack\n",
      "of generalization observed in current systems. Progress\n",
      "towards robust systems with current architectures is likely\n",
      "to require training and measuring performance on a wide\n",
      "range of domains and tasks. Recently, several benchmarks\n",
      "have been proposed such as GLUE (Wang et al., 2018) and\n",
      "decaNLP (McCann et al., 2018) to begin studying this.\n",
      "Multitask learning (Caruana, 1997) is a promising frame-\n",
      "work for improving general performance. However, mul-\n",
      "titask training in NLP is still nascent. Recent work re-\n",
      "ports modest performance improvements (Yogatama et al.,\n",
      "2019) and the two most ambitious efforts to date have\n",
      "trained on a total of 10 and 17 (dataset, objective)\n",
      "pairs respectively (McCann et al., 2018) (Bowman et al.,\n",
      "2018). From a meta-learning perspective, each (dataset,\n",
      "objective) pair is a single training example sampled\n",
      "from the distribution of datasets and objectives. Current\n",
      "ML systems need hundreds to thousands of examples to\n",
      "induce functions which generalize well. This suggests that\n",
      "multitask training many need just as many effective training\n",
      "pairs to realize its promise with current approaches.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 demonstrates the ability of language models to perform a wide range of tasks in a zero-shot setting, achieving promising, competitive, and state-of-the-art results depending on the task.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What are some high-level results of GPT2?\", \n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the source nodes to observe the meta data used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_path': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_type': 'application/pdf', 'file_size': 582775, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n",
      "{'page_label': '2', 'file_name': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_path': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_type': 'application/pdf', 'file_size': 582775, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a Auto Retrieval Tool**\n",
    "Finally lets wraps the retrieval tool into a function that takes in both the query string and page numbers as filters in the same manner we saw in example. \n",
    "- The LLM can infer page numbers to filter for a user query, eliminating the need for manual metadata specification. \n",
    "- Metadata is not limited to page numbers and can include various elements like \n",
    "  - section IDs, \n",
    "  - headers, and \n",
    "  - footers through LlamaIndex abstractions. \n",
    "- The ability to use multiple metadata filters is especially prominent in advanced models like GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT demonstrates the ability of language models to perform a wide range of downstream tasks in a zero-shot setting without any parameter or architecture modification. It showcases promising, competitive, and state-of-the-art results across various tasks, highlighting the potential of language models for multitask learning.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool], \n",
    "    \"What are the high-level results of MetaGPT as described on page 2?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_path': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_type': 'application/pdf', 'file_size': 582775, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n",
      "{'page_label': '2', 'file_name': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_path': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_type': 'application/pdf', 'file_size': 582775, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's add some other tools!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful if you want to get a summary of MetaGPT\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT comparisons with ChatDev are not directly discussed in the provided context. The context mainly focuses on language models, data overlap analysis, generalization versus memorization, and the impact of text similarity on model performance.\n",
      "{'page_label': '8', 'file_name': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_path': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_type': 'application/pdf', 'file_size': 582775, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n",
      "{'page_label': '8', 'file_name': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_path': 'language_models_are_unsupervised_multitask_learners.pdf', 'file_type': 'application/pdf', 'file_size': 582775, 'creation_date': '2024-06-07', 'last_modified_date': '2024-06-07'}\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\", \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"The paper discusses the impact of climate change on biodiversity and ecosystems.\"}\n",
      "=== Function Output ===\n",
      "The paper does not discuss the impact of climate change on biodiversity and ecosystems.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What is a summary of the paper?\", \n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
